# -*- coding: utf-8 -*-
"""ML_Assignment_1002223250_Trupti_Shriyan.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qG_L4zdcSIfNT_ZtM9maKrMBwIhZ-0at
"""

# Importing important PyTorch Modules
import torch
import torch.nn as nn
import torch.optim as optim

# For loading and transforming MNIST data
from torchvision import datasets, transforms

#It helps load data in batches for training/ testing
from torch.utils.data import DataLoader

# Defines LeNet - 5 Convolutional Neural Network Architecture
class LeNet5(nn.Module):
  # Default is 10 for MNIST digits 0-9
    def __init__(self, num_classes=10):
        super(LeNet5, self).__init__()

        # First Convoultional Layer (C1)
        self.layer1 = nn.Sequential(
            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),  # C1 input channel (grayscale): 1 -> 6 filters (feature maps)

            # Activation Function
            nn.Tanh(),

            # S2: Subsampling divides width/ height
            nn.AvgPool2d(kernel_size=2, stride=2)
        )

        # Second Convolutional Layer (C3)
        self.layer2 = nn.Sequential(
            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0), # C3: 6 -> 16 feature maps
            # Activation Function
            nn.Tanh(),
            nn.AvgPool2d(kernel_size=2, stride=2)                 # S4: Subsampling
        )

        # Fully connected layer 1 (input : flattened 16 feature maps of size 5 x 5 = 400)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)                    # FC1: Fully connected layer ( 400 -> 120 neurons)

        # Fully connected layer 2
        self.fc2 = nn.Linear(120, 84)                           # FC2: Fully connected layer ( 120 -> 84 neurons)

        # Output layer (one for each digit class)
        self.fc3 = nn.Linear(84, num_classes)                  # Final layer :  84 -> 10 (digit classes) -- Output layer

    def forward(self, x):

        # Apply forst conv + tanh + pooling
        x = self.layer1(x)

        # Apply second conv + tanh + pooling
        x = self.layer2(x)

        # Flatten from [ batch, 16, 5, 5] --> [batch, 400]
        x = x.view(x.size(0), -1)  # Flatten the tensor

        # FC1 + tanh
        x = torch.tanh(self.fc1(x))

        #FC2 + tanh
        x = torch.tanh(self.fc2(x))

        # Output layer (no activation here, raw logits)
        x = self.fc3(x)
        return x

# Prepare the MNIST dataset

# Tranformations : Resize images and normalize them
transform = transforms.Compose([

    # Resize from 28 x 28 --> 32 x 32 to match LeNet-5 input size
    transforms.Resize((32, 32)),  # Resize to match LeNet-5 input size

    # Convert to Pytorch tensor [0,1]
    transforms.ToTensor(),

    # Normalize to [-1, 1] for tanh
    transforms.Normalize((0.5,), (0.5,))
])

# Download and prepare MNIST traning and test datasets
train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)
test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)

# Load datasets in batches uing Data loaders
train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)

# Check the number of images in the training and test sets
print(f"Number of training images: {len(train_dataset)}")
print(f"Number of test images: {len(test_dataset)}")

print(f"Number of batches in training loader: {len(train_loader)}")
print(f"Number of batches in test loader: {len(test_loader)}")

# Initialize the model, loss function, and optimizer

# Use GPU if available, else fallback to CPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Instantiate the model and move it to the selected device
model = LeNet5().to(device)

# Use cross-entropy loss, which is standard for classification tasks
criterion = nn.CrossEntropyLoss()

#Adam Optimizer is chosen for faster convergence
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop

# Train for 10 full passes over the data
num_epochs = 10
for epoch in range(num_epochs):

    # Set model to training mode
    model.train()

    # To accumulate loss over the epoch
    total_loss = 0

    # Iterate over batches
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)

        # Forward pass
        outputs = model(images)

        # Compute loss
        loss = criterion(outputs, labels)

        # Backward pass and optimization

        # Clear previous gradients
        optimizer.zero_grad()

        #Backpropagation
        loss.backward()

        # Update weights
        optimizer.step()

        #Accumulate batch loss
        total_loss += loss.item()


    # Print average loss for this epoch
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}')

# Testing loop

# Set model to evaluation mode
model.eval()

# Count correct predictions
correct = 0

# Total number of test samples
total = 0

# Disable gradient computation for efficiency
with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)

        # Forward Pass
        outputs = model(images)

        # Get index of max logit  (predicted label)
        _, predicted = torch.max(outputs.data, 1)

        # Update total count
        total += labels.size(0)

        # Count correct predictions
        correct += (predicted == labels).sum().item()

# Calculate accuracy %
accuracy = correct / total * 100

# Print final test accuracy
print(f'Test Accuracy: {accuracy:.2f}%')

